# 2.1 感知机模型

感知机是**二分类**的线性分类模型，输入为实例的特征向量，输出为实例的类别（+1和-1）。感知机对应<u>**将实例划分为两类的分离超平面**</u>。感知机的目的在于求出该超平面，为求得超平面而引入了**损失函数**，利用梯度下降法对损失函数进行最优化。
$$
假设输入空间（特征向量）x∈R^n，输出空间为Y∈\{-1,+1\}
$$

$$
输入x∈X表示实例的特征向量，对应于输入空间的点，输出y∈Y表示实例的类别
$$

由输入空间到输出空间的如下函数：
$$
f(x)=sign(w\cdot x+b)
$$

称为感知机。其中，
$$
w和b是模型参数,w叫做权值，b叫做偏置，w\cdot x表示w和x的内积
$$
sign是符号函数，即
$$
sign(x)=\Big\{\begin{align}
+1,\quad x\geqslant0
\\
-1,\quad x<0
\end{align}
$$

------

>  感知机模型的任务是解决二分类问题，超平面的形式在不同维度的空间中表现形式是不同的，感知机学习的关键是需要如何学习出超平面的参数w，b。

------

# 2.2 感知机学习策略

------

> 损失函数是关于w，b连续可导的，（并非是用误分类点的个数来表示损失函数），可以不断优化。

------

## 2.2.1 数据集的线性可分性

$$
如果存在某个超平面S能够将数据集的正实例点和负实例点完全正确地划分到超平面的两侧，即
$$

$$
对所有 y_i=+1，的实例i，
$$

$$
有 w\cdot x_i+b>0，对所有 y_i=-1 的实例i，
$$

$$
有w\cdot xi+b<0,则称数据集T为线性可分数据集(linearly\quad separable\quad data set);
$$

$$
否则，称数据集T线性不可分。
$$

<img src="https://img-blog.csdnimg.cn/20181229065144827.JPG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ppYW5fcWlhbw==,size_16,color_FFFFFF,t_70" alt="图1 二维平面上的线性可分与线性不可分" style="zoom:67%;" />
$$
图1 \quad二维平面上的线性可分与线性不可分
$$
![图3 投影特定方向实现线性可分](https://img-blog.csdnimg.cn/20181229064426315.gif)
$$
图3 \quad投影特定方向实现线性可分
$$


## 2.2.2 感知机学习策略

假设训练数据集是线性可分的，感知机学习的目标是求得一个能够将训练集正实例点和负实例点完全正确分开的分离超平面。为了找出这样的超平面，即确定感知机模型参数w，b，需要确定一个学习策略，即定义（经验）损失函数并将损失函数极小化。

损失函数的一个自然选择是<u>**误分类点的总数**</u>，但是，这样的损失函数不是参数w，b的连续可导函数，不易优化。损失函数的另一个选择是<u>**误分类点到超平面S的总距离**</u>，这是感知机所采用的。
:smile:
$$
下面为R^n中任一点x_0到超平面S的距离：
$$

$$
-\frac{1}{||w||}|w\cdot x_0+b|
$$

$$
每一个误分类点都满足\quad-y_i(w\cdot x_0+b)>0
$$

$$
总距离为：
$$

$$
L(w,b)=-\frac{1}{||w||}\sum_{x_i∈M}y_i(w\cdot x_i+b)
$$

<img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20220922180317031.png" alt="image-20220922180317031" style="zoom:67%;" />

其中M为误分类点的数目。
$$
若不考虑\frac{1}{||w||}，可以得到损失函数为：
$$

$$
L(w,b)=-\sum_{x_i∈M}y_i(w\cdot x_i+b)
$$

------

> 

------

$$
损失函数L（w，b）是非负的，如果没有误分类点，损失函数值为0。
$$

误分类点越少，误分类点离超平面的距离值越小，损失函数值就越小。

损失函数在误分类时是参数w,b的线性函数，在正确分类时是0

损失函数L(w,b)是连续可导函数。

感知机学习的策略是在假设空间中选取损失函数式最小的模型参数w,b，即感知机模型。

# 2.3 感知机学习算法

## 2.3.1 感知机学习算法的原始形式

完成损失函数极小化问题，使w,b为以下损失函数极小化问题的解
$$
min L(w,b)=-\sum_{x_i∈M}y_i(w\cdot x+b)
$$
![img](file:///C:\Users\lenovo\AppData\Local\Temp\ksohtml8484\wps1.jpg)

感知机学习算法的原始形式为：

- 输入：训练集T，学习率η

- 输出：w,b

- 感知机模型：
  $$
  f(x)=sign(w\cdot x+b)
  $$
  

步骤：

1. 初始化 w0,b0

2. 在训练集中选取数据（x0,y0）

3. 若选取到误分类点，则更新参数：
   $$
   w\gets w+\eta *y_i*x_i\\ b\gets b+\eta*y_i
   $$

4. 转至（2），直到训练集中没有误分类点。

<img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20220923225531017.png" alt="截取自知乎大佬“忆臻”" style="zoom: 67%;" />

<img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20220923225833189.png" alt="截取自知乎大佬“忆臻" style="zoom:67%;" />

## 2.3.2 算法的收敛性

------

> 暂时看不懂

------



## 2.3.3 感知机学习算法的对偶形式

对偶形式的基本想法是，将w和b表示为实例xi和标记yi的线性组合的形式，通过求解其系数而求得w和b。

在感知机学习算法的原始模式中，假设初始值w0,b0均为0，对误分类点（xi,yi）通过
$$
w\gets w+\eta *y_i*x_i\\ b\gets b+\eta*y_i
$$
逐步修改w,b，假设修改了n次，则w,b关于（xi,yi）的增量分别是αiyixi和αiyi，这里αi=ni η，最后学习到的w,b可以表示为
$$
w=\sum^{N}_{i=1}α_iy_ix_i
\\
b=\sum^{N}_{i=1}α_iy_i
$$
这里的αi≥0，i=1,2,…,N,当η=1时，表示第i哥实例点由于误分而进行更新的次数。实例点更新的次数越多，意味着它距离分离超平面越近，也就是说越难正确分类。（这种实例对学习结果影响最大）



感知机学习算法的对偶形式为：

- 输入：数据集T，学习率

- 输出：α，b

- 感知机模型：
  $$
  f(x)=sign(\sum^{N}_{j=1}α_jy_jx_j\cdot x+b)
  $$

步骤：

1. $$
   α\gets 0\quad,\quad b\gets 0\quad ;
   $$

2. 在训练集中选取数据(xi,yi);

3. 如果
   $$
   y_i(\sum^{N}_{j=1}α_jy_jx_j+b)≤0
   \\
   α_i\gets α_i+\eta
   \\
   b\gets b+\eta y_i
   $$

4. 转至（2）直到没有误分类数据。



------

# 第二章总结

很多看不懂的地方:sob:，特别是各种各样的数学公式（还需要加强数学基础学习）。
大致了解了感知机的作用是：对输入实例的某个特征进行二类分类（分类到超分离平面的两侧），并了解了感知机算法的两种形式（原始形式和对偶形式），都是通过更新参数不断学习接近正确分类的情况（超分离平面更新位置使其能够正确将实例点正确分类至超分离平面两侧），但是我对于对偶形式的理解很浅显:sob:，暂时先了解第一种形式。
（没想到模型竟然真的就是用数学函数来表示的，震惊！）💋



参考内容：

[ 感知机算法详解_jian_qiao的博客-CSDN博客_感知机算法](https://blog.csdn.net/jian_qiao/article/details/85346664)

[浅析感知机（一）--模型与学习策略 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/25696112)

[《浅析感知机（二）--学习算法及python代码剖析》 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/25740235)

[《浅析感知机（三）--收敛性证明与对偶形式以及python代码讲解》 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/25880406)





